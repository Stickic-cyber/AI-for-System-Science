 # AI-for-System-Science
 ## 3.27
`backward`（BP算法）揭露学习本质

双层流可能是是复杂系统学习的本质

多巴胺-奖赏系统-反馈

梯度回溯，可以提取神经元的作用

## 4.3
NPLM loss: cross entropy

pytorch: `NN.embed`

negative sampling: 随机生成负样本，

负样本loss函数为：Log(P)

正样本loss函数为：-Log(P)

反向传播算法是最优控制的必然结果

## 4.10
#### 强化学习 Reinforcement Learning
强化学习自带干预和反事实

多主体博弈（强化学习问题，有整体目标函数，或整体博弈支付矩阵）

股票市场：每个投资主体都在适应，只有“做中学”才能应对多主体博弈

Ablation(剥离实验)

Dreamer V3


## 4.24
#### Casual Inference

阶梯：关联——>干预——>反事实(控制变量)

Data —— Bridge(Casual Inference) —— Model

Simpson Paradox

——>混淆变量 (Confounding Variable)是指与 自变量 和 因变量 均相关的变量

![b2e974c277e6750d056cdb61baf2527](https://github.com/user-attachments/assets/bca26c15-87e5-4648-b839-de147188d907)


![image](https://github.com/user-attachments/assets/755901d4-0197-4ba4-a379-e44580b00927)

《The Book Of Why》
